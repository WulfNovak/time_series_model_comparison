{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:/Users/WulfN/Python Projects/time_series_model_comparison')\n",
    "\n",
    "# Functions\n",
    "# %run data_prep_fns/general_scale.py\n",
    "\n",
    "# Multiple Outputs per cell\n",
    "%config interactive_shell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test datasets\n",
    "import pickle\n",
    "\n",
    "with open('datasets/energy_dt.pickle', 'rb') as file:\n",
    "    energy_dt = pickle.load(file)  \n",
    "\n",
    "with open('datasets/glob_pop.pickle', 'rb') as file_2:\n",
    "    glob_pop_dt = pickle.load(file_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', 'T5', 'RH_5',\n",
       "       'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', 'RH_9', 'T_out',\n",
       "       'Press_mm_hg', 'RH_out', 'Windspeed', 'Visibility', 'Tdewpoint', 'rv1',\n",
       "       'rv2', 'total_Wh'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energy_dt.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Split datasets into testing and validation\n",
    "\n",
    "# Energy data\n",
    "x_energy = energy_dt.drop(columns = ['total_Wh'])\n",
    "x_energy_sc = MinMaxScaler().fit_transform(x_energy)\n",
    "y_energy = energy_dt['total_Wh']\n",
    "# y_energy_sc = MinMaxScaler().transform(y_energy) # transform so as not to introduce bias from training set from fit() function the testing set\n",
    "### Test how scaling or NOT scaling y affects the model ###\n",
    "\n",
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x_energy_sc, y_energy, test_size = .2, shuffle = False)\n",
    "\n",
    "# Global Population data\n",
    "x_glob_pop = glob_pop_dt.drop(columns = ['Population'])\n",
    "x_glob_pop_sc = MinMaxScaler().fit_transform(x_glob_pop)\n",
    "y_glob_pop = glob_pop_dt['Population']\n",
    "# y_glob_pop_sc = MinMaxScaler().transform(y_glob_pop)\n",
    "\n",
    "x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_glob_pop_sc, y_glob_pop, test_size = .2, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array\n",
    "window_size = 2 # If hour context, then 6, if day, then 144 (perhaps try 72 has half day, or 24 as 1/6th of day)\n",
    "# (2, 6, 24, 72, 144)\n",
    "### Try lengths of different size: see what DFs of different lengths look like\n",
    "ts_train_windows = timeseries_dataset_from_array(x_train_1, x_train_1, sequence_length=window_size)  # length = 2, 3, 4\n",
    "# ts_train_windows_1 = timeseries_dataset_from_array(x_train_1, y_train_1, sequence_length=window_size)\n",
    "\n",
    "ts_test_windows = timeseries_dataset_from_array(x_test_1, x_test_1, sequence_length=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - loss: 0.1806\n",
      "Epoch 2/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0504\n",
      "Epoch 3/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0485\n",
      "Epoch 4/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - loss: 0.0479\n",
      "Epoch 5/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0476\n",
      "Epoch 6/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 0.0474\n",
      "Epoch 7/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - loss: 0.0472\n",
      "Epoch 8/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 0.0471\n",
      "Epoch 9/50\n",
      "\u001b[1m124/124\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - loss: 0.0470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22509c7dc40>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try 1 with generateTrain set\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "tf.random.set_seed(4)\n",
    "\n",
    "# .as_numpy_iterator()\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, \n",
    "                                            min_delta = .001,\n",
    "                                            restore_best_weights=True,\n",
    "                                            start_from_epoch=5) \n",
    "\n",
    "lstm_1 = tf.keras.Sequential() # assume this creates a 'return_sequences = True' type of things\n",
    "lstm_1.add(tf.keras.layers.LSTM(16, activation='relu', # sigmoid, tanh\n",
    "                                input_shape=(window_size, x_train_1.shape[1]))) # relu may have problem with exploding gradients\n",
    "\n",
    "# lstm_1.add(tf.keras.layers.Dropout(.2)) # try with and without this layer\n",
    "lstm_1.add(tf.keras.layers.Dense(1)) # timedistributed dense instead? (to preserve order)\n",
    "lstm_1.compile(loss='mse', # loss = root_mean_squared_error, loss = 'mse' (no metrics)\n",
    "               optimizer='adam') # Another param: metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "\n",
    "lstm_1.fit(x=ts_train_windows, \n",
    "           shuffle=False,\n",
    "           epochs=50,\n",
    "           callbacks = [callback]) # try different batch sizes\n",
    "\n",
    "# \n",
    "\n",
    "# Seeing about early stopping for epochs\n",
    "# lstm_1.summary()\n",
    "\n",
    "# if using stateful, then lstm_1.fit(stuff, batch_size = something)\n",
    "# lstm_1.reset_states() # reset states after each epoch (necessary for epochs > 1!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try 1 with generateTrain set\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_tuner as kt\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "tf.random.set_seed(4)\n",
    "\n",
    "# https://keras.io/guides/keras_tuner/getting_started/\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, \n",
    "                                            min_delta = .001,\n",
    "                                            restore_best_weights=True,\n",
    "                                            start_from_epoch=5) \n",
    "def lstm_model_build(hp):\n",
    "\n",
    "    model_type = hp.Choice(\"model_type\", [\"base\", \"base_dropout\"]) # \"stacked\", \"stacked_dropout\"\n",
    "\n",
    "    # Perhaps include data window size variations too\n",
    "\n",
    "    if model_type == 'base':\n",
    "        lstm_1 = tf.keras.Sequential() # assume this creates a 'return_sequences = True' type of things\n",
    "        lstm_1.add(tf.keras.layers.LSTM(\n",
    "            hp.Choice('units', [16, 26]), # , 32, 52, 64, 128\n",
    "            activation='relu', # sigmoid, tanh\n",
    "            input_shape=(window_size, x_train_1.shape[1]))) \n",
    "        lstm_1.add(tf.keras.layers.Dense(1))\n",
    "        #return lstm_1\n",
    "\n",
    "    elif model_type == 'base_dropout':\n",
    "        lstm_1 = tf.keras.Sequential() # assume this creates a 'return_sequences = True' type of things\n",
    "        lstm_1.add(tf.keras.layers.LSTM(\n",
    "            hp.Choice('units', [16, 26]), # , 32, 52, 64, 128\n",
    "            activation='relu', \n",
    "            input_shape=(window_size, x_train_1.shape[1]))) \n",
    "        lstm_1.add(tf.keras.layers.Dropout(.2)) # try with and without this layer\n",
    "        lstm_1.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "        lstm_1.compile(loss='mse',\n",
    "                    optimizer='adam') # Another param: metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "        # Learning rate? \n",
    "\n",
    "    # Currently, only 1 epoch is run, need callback within this function in order to ensure early stopping occurs    \n",
    "    return lstm_1\n",
    "\n",
    "tuner = kt.GridSearch( # what is Hyperband\n",
    "    hypermodel=lstm_model_build,\n",
    "    objective=kt.Objective('loss', direction = 'min'),\n",
    "    # max_trials=3, \n",
    "    seed=89,\n",
    "    directory='keras_tuner_dir', \n",
    "    project_name='lstm_1',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(\n",
    "    x=ts_train_windows, \n",
    "    # should require a y set as well, right? \n",
    "    callbacks=[keras.callbacks.EarlyStopping(patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activate_all_conditions',\n",
       " '_api_export_path',\n",
       " '_api_export_symbol_id',\n",
       " '_build_and_fit_model',\n",
       " '_build_hypermodel',\n",
       " '_configure_tensorboard_dir',\n",
       " '_deepcopy_callbacks',\n",
       " '_filter_metrics',\n",
       " '_get_build_config_fname',\n",
       " '_get_checkpoint_fname',\n",
       " '_get_tensorboard_dir',\n",
       " '_get_tuner_fname',\n",
       " '_is_worker',\n",
       " '_override_compile_args',\n",
       " '_populate_initial_space',\n",
       " '_run_and_update_trial',\n",
       " '_try_build',\n",
       " '_try_run_and_update_trial',\n",
       " 'directory',\n",
       " 'distribution_strategy',\n",
       " 'executions_per_trial',\n",
       " 'get_best_hyperparameters',\n",
       " 'get_best_models',\n",
       " 'get_state',\n",
       " 'get_trial_dir',\n",
       " 'hypermodel',\n",
       " 'load_model',\n",
       " 'logger',\n",
       " 'loss',\n",
       " 'max_model_size',\n",
       " 'metrics',\n",
       " 'on_batch_begin',\n",
       " 'on_batch_end',\n",
       " 'on_epoch_begin',\n",
       " 'on_epoch_end',\n",
       " 'on_search_begin',\n",
       " 'on_search_end',\n",
       " 'on_trial_begin',\n",
       " 'on_trial_end',\n",
       " 'optimizer',\n",
       " 'oracle',\n",
       " 'pre_create_trial',\n",
       " 'project_dir',\n",
       " 'project_name',\n",
       " 'reload',\n",
       " 'remaining_trials',\n",
       " 'results_summary',\n",
       " 'run_trial',\n",
       " 'save',\n",
       " 'save_model',\n",
       " 'search',\n",
       " 'search_space_summary',\n",
       " 'seed',\n",
       " 'set_state',\n",
       " 'tuner_id']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tuner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in keras_tuner_dir\\lstm_1\n",
      "Showing 10 best trials\n",
      "Objective(name=\"loss\", direction=\"min\")\n",
      "\n",
      "Trial 0002 summary\n",
      "Hyperparameters:\n",
      "model_type: base_dropout\n",
      "units: 16\n",
      "Score: 0.06586642563343048\n",
      "\n",
      "Trial 0003 summary\n",
      "Hyperparameters:\n",
      "model_type: base_dropout\n",
      "units: 26\n",
      "Score: 0.08675344288349152\n",
      "\n",
      "Trial 0000 summary\n",
      "Hyperparameters:\n",
      "model_type: base\n",
      "units: 16\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 993, in _assert_compile_called\n",
      "    raise ValueError(msg)\n",
      "ValueError: You must call `compile()` before using the model.\n",
      "\n",
      "\n",
      "Trial 0001 summary\n",
      "Hyperparameters:\n",
      "model_type: base\n",
      "units: 26\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 122, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"c:\\Users\\WulfN\\Python Projects\\virtual_env\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py\", line 993, in _assert_compile_called\n",
      "    raise ValueError(msg)\n",
      "ValueError: You must call `compile()` before using the model.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()\n",
    "\n",
    "\n",
    "# score model, record loss\n",
    "# models = []\n",
    "\n",
    "# for model in best_model:\n",
    "#     loss = model.evaluate(ts_test_windows)\n",
    "#     print(\"Loss:\", loss)\n",
    "#best_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_type': 'base_dropout', 'units': 26}\n",
      "{'model_type': 'base_dropout', 'units': 16}\n",
      "{'model_type': 'base', 'units': 16}\n",
      "{'model_type': 'base', 'units': 26}\n"
     ]
    }
   ],
   "source": [
    "grid_num = 4\n",
    "\n",
    "best_hp = tuner.get_best_hyperparameters(grid_num)\n",
    "\n",
    "for i in range(0,grid_num):\n",
    "    print(best_hp[i].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare predictions to actuals\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lstm_1.predict(generateTest), label = 'Predictions') # these need to be rescaled\n",
    "plt.plot(y_test_1, label = 'Actuals')   \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare predictions to actuals\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lstm_1.predict(generateTest), label = 'Predictions') # these need to be rescaled\n",
    "plt.plot(y_test_1, label = 'Actuals')   \n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
