{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow example where M3 data is read in. Use example to work with the energy data\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from datasetsforecast.m3 import M3\n",
    "from datasetsforecast.long_horizon import ECL, ETTm1, LongHorizon\n",
    "import os\n",
    "os.chdir('C:/Users/WulfN/Python Projects/time_series_model_comparison')\n",
    "os.environ['NIXTLA_ID_AS_COL'] = '1'\n",
    "\n",
    "%config interactive_shell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M1</td>\n",
       "      <td>1990-01-31</td>\n",
       "      <td>2640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1</td>\n",
       "      <td>1990-02-28</td>\n",
       "      <td>2640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M1</td>\n",
       "      <td>1990-03-31</td>\n",
       "      <td>2160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M1</td>\n",
       "      <td>1990-04-30</td>\n",
       "      <td>4200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M1</td>\n",
       "      <td>1990-05-31</td>\n",
       "      <td>3360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167557</th>\n",
       "      <td>M999</td>\n",
       "      <td>1993-10-31</td>\n",
       "      <td>5225.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167558</th>\n",
       "      <td>M999</td>\n",
       "      <td>1993-11-30</td>\n",
       "      <td>5236.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167559</th>\n",
       "      <td>M999</td>\n",
       "      <td>1993-12-31</td>\n",
       "      <td>5186.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167560</th>\n",
       "      <td>M999</td>\n",
       "      <td>1994-01-31</td>\n",
       "      <td>5143.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167561</th>\n",
       "      <td>M999</td>\n",
       "      <td>1994-02-28</td>\n",
       "      <td>5152.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>167562 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_id         ds       y\n",
       "0             M1 1990-01-31  2640.0\n",
       "1             M1 1990-02-28  2640.0\n",
       "2             M1 1990-03-31  2160.0\n",
       "3             M1 1990-04-30  4200.0\n",
       "4             M1 1990-05-31  3360.0\n",
       "...          ...        ...     ...\n",
       "167557      M999 1993-10-31  5225.9\n",
       "167558      M999 1993-11-30  5236.3\n",
       "167559      M999 1993-12-31  5186.6\n",
       "167560      M999 1994-01-31  5143.4\n",
       "167561      M999 1994-02-28  5152.6\n",
       "\n",
       "[167562 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_m3, *_= M3.load(directory = '/datasets', group = 'Monthly') # ... Where is the rest of the data, why would you group on unique id\n",
    "y_m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with univariate\n",
    "horizon = 18\n",
    "\n",
    "y1 = y_m3.groupby('unique_id').tail(horizon) \n",
    "x1 = y_m3.drop(y1.index).reset_index(drop=True)\n",
    "\n",
    "y2 = y_m3.groupby('unique_id').tail(horizon) \n",
    "x2 = y_m3.drop(y2.index)\n",
    "\n",
    "y2 = y2.reset_index(level=0) # index col needed for TimeMixer\n",
    "x2 = x2.reset_index(level=0) # maybe reset to make it sequentual per unique id? \n",
    "# example drops index for x, consider trying that\n",
    "\n",
    "# OR NIXTLA_ID_AS_COL in environment variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECL_dt[1]['ds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 1\n",
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "   | Name              | Type                 | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0  | loss              | MAE                  | 0      | train\n",
      "1  | padder            | ConstantPad1d        | 0      | train\n",
      "2  | scaler            | TemporalNorm         | 0      | train\n",
      "3  | pdm_blocks        | ModuleList           | 29.2 K | train\n",
      "4  | preprocess        | SeriesDecomp         | 0      | train\n",
      "5  | enc_embedding     | DataEmbedding_wo_pos | 2.5 K  | train\n",
      "6  | normalize_layers  | ModuleList           | 4      | train\n",
      "7  | predict_layers    | ModuleList           | 1.0 K  | train\n",
      "8  | projection_layer  | Linear               | 33     | train\n",
      "9  | out_res_layers    | ModuleList           | 1.7 K  | train\n",
      "10 | regression_layers | ModuleList           | 1.0 K  | train\n",
      "--------------------------------------------------------------------\n",
      "33.1 K    Trainable params\n",
      "2.4 K     Non-trainable params\n",
      "35.5 K    Total params\n",
      "0.142     Total estimated model params size (MB)\n",
      "138       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab0ceb1dfd84a188a5203e5e28d84af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a8cd3c3bfc64f38b4ff077780b43da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b6363ec980460b831180aa99f5793f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d5fcfb95e34a2c9bfe79bd4e0a2ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cf72a0b5eb403da8a577e4d2b09d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da753e199306417db3b81f405fad735e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195b2e4e243a4414b0eadb9b279682bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61aa6e936e249098d7c6d875e41b534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166369bc69d0428fb59d5364f85901de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcd48cfb07046c0aa1f7b240bd16957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9677f5dee9184a7299d5a897b1a37ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d91acfb9d4334a3f41a0fd7167aab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO: Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO: GPU available: False, used: False\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd709e85bc7747b4aa02d986ad72ebf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MAE, MSE\n",
    "from neuralforecast.models import TimeMixer\n",
    "\n",
    "simple_tm_model = TimeMixer(input_size=2*horizon, \n",
    "                        h=horizon, \n",
    "                        n_series=1, \n",
    "                        scaler_type='identity', \n",
    "                        early_stop_patience_steps=3)\n",
    "\n",
    "nf = NeuralForecast(models=[simple_tm_model], freq='M')\n",
    "\n",
    "\n",
    "# results, then loop through each of the data sets\n",
    "start = time.time()\n",
    "\n",
    "nf.fit(x1, val_size=horizon) # perhaps alg also validates\n",
    "preds = nf.predict()\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = round(end - start,0)\n",
    "\n",
    "# this model is really slow on this simple dataset!\n",
    "# number of params is very large\n",
    "# Info as model trains is unusual\n",
    "    # x2 = 332 seconds (also, no gpu computer)\n",
    "    # x2 = stops at max_steps = 1000\n",
    "    # x1 = 283\n",
    "    # x1 = stops at max_steps = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['2016-07-01 02:00:00', '2016-07-01 03:00:00', '2016-07-01 04:00:00',\\n       '2016-07-01 05:00:00', '2016-07-01 06:00:00', '2016-07-01 07:00:00',\\n       '2016-07-01 08:00:00', '2016-07-01 09:00:00', '2016-07-01 10:00:00',\\n       '2016-07-01 11:00:00',\\n       ...\\n       '2019-07-01 16:00:00', '2019-07-01 17:00:00', '2019-07-01 18:00:00',\\n       '2019-07-01 19:00:00', '2019-07-01 20:00:00', '2019-07-01 21:00:00',\\n       '2019-07-01 22:00:00', '2019-07-01 23:00:00', '2019-07-02 00:00:00',\\n       '2019-07-02 01:00:00'],\\n      dtype='object', length=8443584)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[74], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m index_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ECL_dt[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;66;03m# vector of index vals\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# ts_ECL_dt = TimeSeriesDataset(ECL_dt, temporal=, temporal_cols=)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m ts_ECL_dt_torch \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeriesDataSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mECL_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# dt_x sets date as index, which may not be what's needed)\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mECL_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;66;43;03m# dt_x.index, \u001b[39;49;00m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mECL_dt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mgroup_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munique_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;66;43;03m# list of all columns OR an id and time column? \u001b[39;49;00m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:350\u001b[0m, in \u001b[0;36mTimeSeriesDataSet.__init__\u001b[1;34m(self, data, time_idx, target, group_ids, weight, max_encoder_length, min_encoder_length, min_prediction_idx, min_prediction_length, max_prediction_length, static_categoricals, static_reals, time_varying_known_categoricals, time_varying_known_reals, time_varying_unknown_categoricals, time_varying_unknown_reals, variable_groups, constant_fill_strategy, allow_missing_timesteps, lags, add_relative_time_idx, add_target_scales, add_encoder_length, target_normalizer, categorical_encoders, scalers, randomize_length, predict_mode)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_prediction_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin prediction length must be larger than 0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_prediction_length, \u001b[38;5;28mint\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin prediction length must be integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtime_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeseries index should be of type integer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget \u001b[38;5;241m=\u001b[39m target\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m weight\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\pandas\\core\\frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['2016-07-01 02:00:00', '2016-07-01 03:00:00', '2016-07-01 04:00:00',\\n       '2016-07-01 05:00:00', '2016-07-01 06:00:00', '2016-07-01 07:00:00',\\n       '2016-07-01 08:00:00', '2016-07-01 09:00:00', '2016-07-01 10:00:00',\\n       '2016-07-01 11:00:00',\\n       ...\\n       '2019-07-01 16:00:00', '2019-07-01 17:00:00', '2019-07-01 18:00:00',\\n       '2019-07-01 19:00:00', '2019-07-01 20:00:00', '2019-07-01 21:00:00',\\n       '2019-07-01 22:00:00', '2019-07-01 23:00:00', '2019-07-02 00:00:00',\\n       '2019-07-02 01:00:00'],\\n      dtype='object', length=8443584)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from neuralforecast.tsdataset import TimeSeriesDataset\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "# Load ECL data\n",
    "ECL_dt = LongHorizon.load( # get ECL data\n",
    "    directory=\"/datasets\", group=\"ECL\")\n",
    "\n",
    "# TimeSeriesDataset (temporal, temporal_cols, indptr, max_size:int,\n",
    "#                     min_size:int, y_idx:int, static=None,\n",
    "#                     static_cols=None, sorted=False)\n",
    "\n",
    "# Maybe combine ECL_dt? \n",
    "\n",
    "# dt_x = ECL_dt[1]\n",
    "# dt_x['ds'] = pd.to_datetime(dt_x['ds'])\n",
    "# dt_x = dt_x.set_index('ds')\n",
    "\n",
    "index_vals = list(ECL_dt[1].index) # vector of index vals\n",
    "\n",
    "# ts_ECL_dt = TimeSeriesDataset(ECL_dt, temporal=, temporal_cols=)\n",
    "ts_ECL_dt_torch = TimeSeriesDataSet(ECL_dt[1], # dt_x sets date as index, which may not be what's needed)\n",
    "                                    time_idx = ECL_dt[1]['ds'],\n",
    "                                    # dt_x.index, \n",
    "                                    target = ECL_dt[0]['y'], \n",
    "                                    group_ids = ['unique_id', 'ds']# list of all columns OR an id and time column? \n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          2016-07-01 02:00:00\n",
       "1          2016-07-01 03:00:00\n",
       "2          2016-07-01 04:00:00\n",
       "3          2016-07-01 05:00:00\n",
       "4          2016-07-01 06:00:00\n",
       "                  ...         \n",
       "8443579    2019-07-01 21:00:00\n",
       "8443580    2019-07-01 22:00:00\n",
       "8443581    2019-07-01 23:00:00\n",
       "8443582    2019-07-02 00:00:00\n",
       "8443583    2019-07-02 01:00:00\n",
       "Name: ds, Length: 8443584, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ECL_dt[1]['ds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ECL_dt[0]\n",
    "x = ECL_dt[1]\n",
    "\n",
    "horizon_2 = 24 * 30\n",
    "\n",
    "test_df = y.groupby('unique_id').tail(horizon_2) # 4 month horizon 24 * 30 * 4 = 2880\n",
    "train_df = x.drop(test_df.index).reset_index(drop=True) # these are only the training y values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>ex_1</th>\n",
       "      <th>ex_2</th>\n",
       "      <th>ex_3</th>\n",
       "      <th>ex_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-01 02:00:00</td>\n",
       "      <td>-0.413043</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-01 03:00:00</td>\n",
       "      <td>-0.369565</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-01 04:00:00</td>\n",
       "      <td>-0.326087</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-01 05:00:00</td>\n",
       "      <td>-0.282609</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-01 06:00:00</td>\n",
       "      <td>-0.239130</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.001370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212459</th>\n",
       "      <td>OT</td>\n",
       "      <td>2019-06-01 21:00:00</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.086301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212460</th>\n",
       "      <td>OT</td>\n",
       "      <td>2019-06-01 22:00:00</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.086301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212461</th>\n",
       "      <td>OT</td>\n",
       "      <td>2019-06-01 23:00:00</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.086301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212462</th>\n",
       "      <td>OT</td>\n",
       "      <td>2019-06-02 00:00:00</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.083562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212463</th>\n",
       "      <td>OT</td>\n",
       "      <td>2019-06-02 01:00:00</td>\n",
       "      <td>-0.456522</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.083562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8212464 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        unique_id                   ds      ex_1      ex_2      ex_3      ex_4\n",
       "0               0  2016-07-01 02:00:00 -0.413043  0.166667 -0.500000 -0.001370\n",
       "1               0  2016-07-01 03:00:00 -0.369565  0.166667 -0.500000 -0.001370\n",
       "2               0  2016-07-01 04:00:00 -0.326087  0.166667 -0.500000 -0.001370\n",
       "3               0  2016-07-01 05:00:00 -0.282609  0.166667 -0.500000 -0.001370\n",
       "4               0  2016-07-01 06:00:00 -0.239130  0.166667 -0.500000 -0.001370\n",
       "...           ...                  ...       ...       ...       ...       ...\n",
       "8212459        OT  2019-06-01 21:00:00  0.413043  0.333333 -0.500000 -0.086301\n",
       "8212460        OT  2019-06-01 22:00:00  0.456522  0.333333 -0.500000 -0.086301\n",
       "8212461        OT  2019-06-01 23:00:00  0.500000  0.333333 -0.500000 -0.086301\n",
       "8212462        OT  2019-06-02 00:00:00 -0.500000  0.500000 -0.466667 -0.083562\n",
       "8212463        OT  2019-06-02 01:00:00 -0.456522  0.500000 -0.466667 -0.083562\n",
       "\n",
       "[8212464 rows x 6 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I don't think marco's example uses multivariate data.\n",
    "## How does the df need to be labeled to work with the neuralforecast model\n",
    "\n",
    "# train_df['ds'] = pd.to_datetime(train_df['ds'])\n",
    "# train_df = train_df.set_index('ds')\n",
    "\n",
    "test_df # 231120  rows\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 1\n"
     ]
    }
   ],
   "source": [
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.models import TimeMixer\n",
    "\n",
    "n_series = train_df.shape[1]\n",
    "\n",
    "ts_model = TimeMixer(\n",
    "    input_size=horizon_2 * n_series, \n",
    "        # If window size, then try values like 144 (day), 1008 (week), 4320 (month)\n",
    "    h=horizon_2, # horizon, 15788 of 10 minute steps\n",
    "    n_series=n_series,\n",
    "    d_model=16, #dimension of the model, default 32\n",
    "    # d_ff= #dimension of fully connected network, default 32\n",
    "    down_sampling_layers=3, # param search\n",
    "    down_sampling_window=2, # param search\n",
    "    learning_rate=0.01, # param search\n",
    "    scaler_type='robust',\n",
    "    # dropout rate\n",
    "    early_stop_patience_steps=3,\n",
    "    max_steps=100,\n",
    "    loss=MAE(),\n",
    "    #valid_loss=MAE(),\n",
    "    batch_size=n_series\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nf \u001b[38;5;241m=\u001b[39m NeuralForecast(models\u001b[38;5;241m=\u001b[39m[ts_model], freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhorizon_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m preds \u001b[38;5;241m=\u001b[39m nf\u001b[38;5;241m.\u001b[39mpredict()\n\u001b[0;32m      4\u001b[0m preds\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\neuralforecast\\core.py:485\u001b[0m, in \u001b[0;36mNeuralForecast.fit\u001b[1;34m(self, df, static_df, val_size, sort_df, use_init_models, verbose, id_col, time_col, target_col, distributed_config)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, (pd\u001b[38;5;241m.\u001b[39mDataFrame, pl_DataFrame)):\n\u001b[0;32m    484\u001b[0m     validate_freq(df[time_col], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq)\n\u001b[1;32m--> 485\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_dates, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatic_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatic_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredict_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mid_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_df \u001b[38;5;241m=\u001b[39m sort_df\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, SparkDataFrame):\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\neuralforecast\\core.py:301\u001b[0m, in \u001b[0;36mNeuralForecast._prepare_fit\u001b[1;34m(self, df, static_df, sort_df, predict_only, id_col, time_col, target_col)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_col \u001b[38;5;241m=\u001b[39m time_col\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_col \u001b[38;5;241m=\u001b[39m target_col\n\u001b[1;32m--> 301\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_nan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m dataset, uids, last_dates, ds \u001b[38;5;241m=\u001b[39m TimeSeriesDataset\u001b[38;5;241m.\u001b[39mfrom_df(\n\u001b[0;32m    304\u001b[0m     df\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m    305\u001b[0m     static_df\u001b[38;5;241m=\u001b[39mstatic_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     target_col\u001b[38;5;241m=\u001b[39mtarget_col,\n\u001b[0;32m    310\u001b[0m )\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m predict_only:\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\neuralforecast\\core.py:330\u001b[0m, in \u001b[0;36mNeuralForecast._check_nan\u001b[1;34m(self, df, static_df, id_col, time_col, target_col)\u001b[0m\n\u001b[0;32m    328\u001b[0m df_to_check \u001b[38;5;241m=\u001b[39m ufp\u001b[38;5;241m.\u001b[39mfilter_with_mask(df, available_mask)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m temporal_cols:\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ufp\u001b[38;5;241m.\u001b[39mis_nan_or_none(\u001b[43mdf_to_check\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m    331\u001b[0m         cols_with_nans\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m static_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\WulfN\\Python Projects\\learn_ts\\ts_env_310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'y'"
     ]
    }
   ],
   "source": [
    "nf = NeuralForecast(models=[ts_model], freq='M')\n",
    "nf.fit(df=train_df, val_size=horizon_2 #, y = ?, targets? \n",
    "       ) # need neuralforecast timeseriesDataset object\n",
    "preds = nf.predict()\n",
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_env_310",
   "language": "python",
   "name": "ts_env_310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
